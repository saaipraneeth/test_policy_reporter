{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook demonstrates a generic Resume Parser solution that:\n",
    "- Supports PDF and Word document formats\n",
    "- Extracts structured information (name, email, skills)\n",
    "- Uses latest Google GenAI SDK (google-genai package)\n",
    "- Returns data in JSON format\n",
    "\n",
    "## Architecture\n",
    "1. **Document Processing**: Extract text from PDF/Word files\n",
    "2. **LLM Parsing**: Use Google's latest GenAI SDK to extract structured data\n",
    "3. **Validation**: Ensure extracted data meets quality standards\n",
    "4. **Evaluation**: Measure parser performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q pypdf2 python-docx google-genai python-dotenv pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded successfully!\n",
      "Using latest Google GenAI SDK\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Optional, Union\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Document processing\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "\n",
    "# LLM and API - Using latest Google GenAI SDK\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Data processing and evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Dependencies loaded successfully!\")\n",
    "print(\"Using latest Google GenAI SDK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Models & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded. Using model: gemini-2.0-flash-exp\n",
      "API Key configured: Yes\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ResumeData:\n",
    "    \"\"\"Data model for parsed resume information\"\"\"\n",
    "    name: str\n",
    "    email: str\n",
    "    skills: List[str]\n",
    "    \n",
    "    def to_json(self) -> str:\n",
    "        \"\"\"Convert to JSON string\"\"\"\n",
    "        return json.dumps(asdict(self), indent=2)\n",
    "    \n",
    "    def validate(self) -> bool:\n",
    "        \"\"\"Validate extracted data\"\"\"\n",
    "        # Basic email validation\n",
    "        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "        email_valid = bool(re.match(email_pattern, self.email)) if self.email else False\n",
    "        \n",
    "        # Name validation (at least 2 characters)\n",
    "        name_valid = len(self.name) >= 2 if self.name else False\n",
    "        \n",
    "        # Skills validation (at least one skill)\n",
    "        skills_valid = len(self.skills) > 0\n",
    "        \n",
    "        return email_valid and name_valid and skills_valid\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration settings\"\"\"\n",
    "    # API Configuration - Using GOOGLE_API_KEY for latest SDK\n",
    "    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY', os.getenv('GEMINI_API_KEY', ''))\n",
    "    MODEL_NAME = 'gemini-2.0-flash-exp'\n",
    "    \n",
    "    # Parsing Configuration\n",
    "    MAX_RETRIES = 3\n",
    "    TEMPERATURE = 0.1  # Low temperature for consistent parsing,\n",
    "    \n",
    "    # File paths\n",
    "    SAMPLE_DIR = Path('sample_resumes')\n",
    "    \n",
    "config = Config()\n",
    "print(f\"Configuration loaded. Using model: {config.MODEL_NAME}\")\n",
    "print(f\"API Key configured: {'Yes' if config.GOOGLE_API_KEY else 'No (will use fallback parser)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document extractor initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "class DocumentExtractor:\n",
    "    \"\"\"Extract text from various document formats\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_pdf(file_path: Union[str, Path]) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from PDF file\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to PDF file\n",
    "            \n",
    "        Returns:\n",
    "            Extracted text as string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            text = \"\"\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                num_pages = len(pdf_reader.pages)\n",
    "                \n",
    "                for page_num in range(num_pages):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    text += page.extract_text()\n",
    "                    \n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting PDF {file_path}: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_docx(file_path: Union[str, Path]) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from Word document\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to Word document\n",
    "            \n",
    "        Returns:\n",
    "            Extracted text as string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            doc = Document(file_path)\n",
    "            text = \"\"\n",
    "            \n",
    "            for paragraph in doc.paragraphs:\n",
    "                text += paragraph.text + \"\\n\"\n",
    "                \n",
    "            # Also extract text from tables\n",
    "            for table in doc.tables:\n",
    "                for row in table.rows:\n",
    "                    for cell in row.cells:\n",
    "                        text += cell.text + \" \"\n",
    "                    text += \"\\n\"\n",
    "                    \n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting Word document {file_path}: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text(file_path: Union[str, Path]) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from file based on extension\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to document\n",
    "            \n",
    "        Returns:\n",
    "            Extracted text as string\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        extension = file_path.suffix.lower()\n",
    "        \n",
    "        if extension == '.pdf':\n",
    "            return DocumentExtractor.extract_from_pdf(file_path)\n",
    "        elif extension in ['.docx', '.doc']:\n",
    "            return DocumentExtractor.extract_from_docx(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {extension}\")\n",
    "\n",
    "# Test the extractor\n",
    "extractor = DocumentExtractor()\n",
    "print(\"Document extractor initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gemini LLM-Based Resume Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google GenAI client initialized with model: gemini-2.0-flash-exp\n",
      "Resume Parser initialized with latest Google GenAI SDK!\n"
     ]
    }
   ],
   "source": [
    "class GeminiResumeParser:\n",
    "    \"\"\"Parse resume text using latest Google GenAI SDK\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None, model: str = 'gemini-2.0-flash-exp'):\n",
    "        \"\"\"\n",
    "        Initialize the Gemini parser with latest SDK\n",
    "        \n",
    "        Args:\n",
    "            api_key: Google API key (or use environment variable)\n",
    "            model: Model name to use\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or config.GOOGLE_API_KEY\n",
    "        self.model_name = model\n",
    "        self.client = None\n",
    "        \n",
    "        if self.api_key:\n",
    "            # Set API key in environment for the SDK\n",
    "            os.environ['GOOGLE_API_KEY'] = self.api_key\n",
    "            # Initialize the client with latest SDK pattern\n",
    "            self.client = genai.Client()\n",
    "            print(f\"Google GenAI client initialized with model: {self.model_name}\")\n",
    "        else:\n",
    "            print(\"No API key provided. Will use fallback parser.\")\n",
    "    \n",
    "    def create_prompt(self, resume_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Create a structured prompt for Gemini\n",
    "        \n",
    "        Args:\n",
    "            resume_text: Raw resume text\n",
    "            \n",
    "        Returns:\n",
    "            Formatted prompt string\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"You are a resume parsing expert. Extract the following information from the resume text below:\n",
    "\n",
    "1. Full name of the candidate (not job titles)\n",
    "2. Email address\n",
    "3. List of technical and professional skills\n",
    "\n",
    "Return the information in this EXACT JSON format only:\n",
    "{{\n",
    "    \"name\": \"Full Name\",\n",
    "    \"email\": \"email@example.com\",\n",
    "    \"skills\": [\"skill1\", \"skill2\", \"skill3\"]\n",
    "}}\n",
    "\n",
    "Important guidelines:\n",
    "- Extract the actual person's name, not their job title or position\n",
    "- Only include valid email addresses with @ symbol\n",
    "- For skills, include programming languages, frameworks, tools, technologies, and relevant soft skills\n",
    "- If any field is not found, use empty string for name/email or empty list for skills\n",
    "- Return ONLY the JSON object, no additional text, no markdown formatting\n",
    "- Do not include ```json or ``` markers\n",
    "\n",
    "Resume text:\n",
    "{resume_text[:3000]}  \n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def parse_with_gemini(self, resume_text: str) -> Optional[ResumeData]:\n",
    "        \"\"\"\n",
    "        Parse resume using latest Google GenAI SDK\n",
    "        \n",
    "        Args:\n",
    "            resume_text: Raw resume text\n",
    "            \n",
    "        Returns:\n",
    "            ResumeData object or None if parsing fails\n",
    "        \"\"\"\n",
    "        if not self.client:\n",
    "            print(\"GenAI client not configured. Using fallback parser.\")\n",
    "            return self.fallback_parser(resume_text)\n",
    "        \n",
    "        try:\n",
    "            prompt = self.create_prompt(resume_text)\n",
    "            \n",
    "            # Use the latest SDK pattern from pmi-logistics\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model_name,\n",
    "                contents=prompt,\n",
    "                config={\n",
    "                    \"temperature\": config.TEMPERATURE,\n",
    "                    \"max_output_tokens\": 500,\n",
    "                    \"top_p\": 0.9,\n",
    "                    \"top_k\": 1\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Extract text from response\n",
    "            response_text = response.text.strip() if hasattr(response, 'text') else str(response).strip()\n",
    "            \n",
    "            # Clean up response text to extract JSON\n",
    "            # Remove markdown code blocks if present\n",
    "            if '```json' in response_text:\n",
    "                response_text = response_text.split('```json')[1].split('```')[0].strip()\n",
    "            elif '```' in response_text:\n",
    "                response_text = response_text.split('```')[1].split('```')[0].strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            data = json.loads(response_text)\n",
    "            \n",
    "            return ResumeData(\n",
    "                name=data.get('name', ''),\n",
    "                email=data.get('email', ''),\n",
    "                skills=data.get('skills', [])\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Gemini parsing error: {str(e)}\")\n",
    "            print(\"Falling back to regex parser...\")\n",
    "            return self.fallback_parser(resume_text)\n",
    "    \n",
    "    def fallback_parser(self, resume_text: str) -> ResumeData:\n",
    "        \"\"\"\n",
    "        Fallback regex-based parser for when Gemini is unavailable\n",
    "        \n",
    "        Args:\n",
    "            resume_text: Raw resume text\n",
    "            \n",
    "        Returns:\n",
    "            ResumeData object with extracted information\n",
    "        \"\"\"\n",
    "        # Extract email\n",
    "        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "        emails = re.findall(email_pattern, resume_text)\n",
    "        email = emails[0] if emails else ''\n",
    "        \n",
    "        # Extract name (heuristic: first line or line before email)\n",
    "        lines = resume_text.split('\\n')\n",
    "        name = ''\n",
    "        for i, line in enumerate(lines[:10]):  # Check first 10 lines\n",
    "            line = line.strip()\n",
    "            if line and not any(char.isdigit() for char in line[:5]):\n",
    "                # Likely a name if it's not empty and doesn't start with numbers\n",
    "                if len(line.split()) <= 4:  # Names usually have 2-4 parts\n",
    "                    name = line\n",
    "                    break\n",
    "        \n",
    "        # Extract skills (common programming languages and tools)\n",
    "        skill_keywords = [\n",
    "            'Python', 'Java', 'JavaScript', 'TypeScript', 'C++', 'C#', 'Ruby', 'Go', 'Rust', 'Swift', 'Kotlin',\n",
    "            'HTML', 'CSS', 'SQL', 'NoSQL', 'MongoDB', 'PostgreSQL', 'MySQL', 'Redis', 'Elasticsearch',\n",
    "            'React', 'Angular', 'Vue.js', 'Node.js', 'Express', 'Django', 'Flask', 'FastAPI', 'Spring Boot',\n",
    "            'Docker', 'Kubernetes', 'AWS', 'Azure', 'GCP', 'Terraform', 'Ansible',\n",
    "            'Machine Learning', 'Deep Learning', 'Data Science', 'AI', 'NLP', 'Computer Vision',\n",
    "            'Git', 'Jenkins', 'CI/CD', 'Agile', 'Scrum', 'REST API', 'GraphQL',\n",
    "            'TensorFlow', 'PyTorch', 'Scikit-learn', 'Pandas', 'NumPy',\n",
    "            'Linux', 'Bash', 'PowerShell', 'Microservices', 'DevOps'\n",
    "        ]\n",
    "        \n",
    "        skills = []\n",
    "        resume_lower = resume_text.lower()\n",
    "        for skill in skill_keywords:\n",
    "            if skill.lower() in resume_lower:\n",
    "                skills.append(skill)\n",
    "        \n",
    "        return ResumeData(name=name, email=email, skills=skills)\n",
    "\n",
    "# Initialize parser\n",
    "parser = GeminiResumeParser()\n",
    "print(\"Resume Parser initialized with latest Google GenAI SDK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Resume Parser Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google GenAI client initialized with model: gemini-2.0-flash-exp\n",
      "Main Resume Parser initialized!\n"
     ]
    }
   ],
   "source": [
    "class ResumeParser:\n",
    "    \"\"\"Main resume parser combining all components\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.extractor = DocumentExtractor()\n",
    "        self.parser = GeminiResumeParser()\n",
    "        self.results = []\n",
    "    \n",
    "    def parse_resume(self, file_path: Union[str, Path]) -> Dict:\n",
    "        \"\"\"\n",
    "        Parse a single resume file\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to resume file\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with parsed data and metadata\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        result = {\n",
    "            'file_name': file_path.name,\n",
    "            'file_path': str(file_path),\n",
    "            'success': False,\n",
    "            'data': None,\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Extract text\n",
    "            print(f\"Processing: {file_path.name}\")\n",
    "            text = self.extractor.extract_text(file_path)\n",
    "            \n",
    "            if not text:\n",
    "                raise ValueError(\"No text extracted from document\")\n",
    "            \n",
    "            # Parse with Gemini\n",
    "            resume_data = self.parser.parse_with_gemini(text)\n",
    "            \n",
    "            if resume_data and resume_data.validate():\n",
    "                result['success'] = True\n",
    "                result['data'] = asdict(resume_data)\n",
    "                print(f\"✓ Successfully parsed: {file_path.name}\")\n",
    "            else:\n",
    "                result['error'] = \"Validation failed\"\n",
    "                if resume_data:\n",
    "                    result['data'] = asdict(resume_data)  # Store partial data\n",
    "                print(f\"✗ Validation failed: {file_path.name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            result['error'] = str(e)\n",
    "            print(f\"✗ Error processing {file_path.name}: {str(e)}\")\n",
    "        \n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def parse_directory(self, directory: Union[str, Path]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Parse all resumes in a directory\n",
    "        \n",
    "        Args:\n",
    "            directory: Path to directory containing resumes\n",
    "            \n",
    "        Returns:\n",
    "            List of parsing results\n",
    "        \"\"\"\n",
    "        directory = Path(directory)\n",
    "        results = []\n",
    "        \n",
    "        # Find all PDF and Word files\n",
    "        files = list(directory.glob('*.pdf')) + list(directory.glob('*.docx'))\n",
    "        \n",
    "        print(f\"Found {len(files)} resume files in {directory}\\n\")\n",
    "        \n",
    "        for file_path in files:\n",
    "            result = self.parse_resume(file_path)\n",
    "            results.append(result)\n",
    "            print()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get parsing statistics\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with statistics\n",
    "        \"\"\"\n",
    "        total = len(self.results)\n",
    "        successful = sum(1 for r in self.results if r['success'])\n",
    "        \n",
    "        return {\n",
    "            'total_files': total,\n",
    "            'successful': successful,\n",
    "            'failed': total - successful,\n",
    "            'success_rate': (successful / total * 100) if total > 0 else 0\n",
    "        }\n",
    "\n",
    "# Initialize main parser\n",
    "main_parser = ResumeParser()\n",
    "print(\"Main Resume Parser initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing with sample resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing resumes from sample_resumes directory...\n",
      "\n",
      "==================================================\n",
      "Found 4 resume files in sample_resumes\n",
      "\n",
      "Processing: MathangiC_Resume.pdf\n",
      "✓ Successfully parsed: MathangiC_Resume.pdf\n",
      "\n",
      "Processing: N_Anusha_Resume.pdf\n",
      "✓ Successfully parsed: N_Anusha_Resume.pdf\n",
      "\n",
      "Processing: dan_bulldog.pdf\n",
      "✓ Successfully parsed: dan_bulldog.pdf\n",
      "\n",
      "Processing: RahulRam_Chandrasekaran-Resume-2022.pdf\n",
      "✓ Successfully parsed: RahulRam_Chandrasekaran-Resume-2022.pdf\n",
      "\n",
      "==================================================\n",
      "\n",
      "Parsing Results Summary:\n",
      "total_files: 4\n",
      "successful: 4\n",
      "failed: 0\n",
      "success_rate: 100.0\n"
     ]
    }
   ],
   "source": [
    "# Parse all resumes in the sample directory\n",
    "print(\"Parsing resumes from sample_resumes directory...\\n\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if config.SAMPLE_DIR.exists():\n",
    "    results = main_parser.parse_directory(config.SAMPLE_DIR)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nParsing Results Summary:\")\n",
    "    stats = main_parser.get_statistics()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(f\"Directory {config.SAMPLE_DIR} not found.\")\n",
    "    print(\"Please add resume files (PDF or DOCX) to the 'sample_resumes' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics & Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PARSER EVALUATION REPORT\n",
      "============================================================\n",
      "\n",
      " Overall Performance:\n",
      "  • Total Files Processed: 4\n",
      "  • Parse Success Rate: 100.0%\n",
      "\n",
      " Field Extraction Rates:\n",
      "  • Name Extraction: 100.0%\n",
      "  • Email Extraction: 100.0%\n",
      "  • Skills Extraction: 100.0%\n",
      "\n",
      " Skills Analysis:\n",
      "  • Average Skills per Resume: 28.2\n",
      "  • Maximum Skills Extracted: 41\n",
      "  • Minimum Skills Extracted: 15\n",
      "\n",
      "============================================================\n",
      "\n",
      " Detailed Results:\n",
      "                              file_name  success  has_name  has_email  num_skills\n",
      "                   MathangiC_Resume.pdf     True      True       True          38\n",
      "                    N_Anusha_Resume.pdf     True      True       True          41\n",
      "                        dan_bulldog.pdf     True      True       True          15\n",
      "RahulRam_Chandrasekaran-Resume-2022.pdf     True      True       True          19\n"
     ]
    }
   ],
   "source": [
    "class ParserEvaluator:\n",
    "    \"\"\"Evaluate parser performance\"\"\"\n",
    "    \n",
    "    def __init__(self, results: List[Dict]):\n",
    "        self.results = results\n",
    "        self.df = self._create_dataframe()\n",
    "    \n",
    "    def _create_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Convert results to DataFrame for analysis\"\"\"\n",
    "        data = []\n",
    "        for result in self.results:\n",
    "            row = {\n",
    "                'file_name': result['file_name'],\n",
    "                'success': result['success'],\n",
    "                'has_name': False,\n",
    "                'has_email': False,\n",
    "                'has_skills': False,\n",
    "                'num_skills': 0,\n",
    "                'error': result.get('error', '')\n",
    "            }\n",
    "            \n",
    "            if result['data']:\n",
    "                row['has_name'] = bool(result['data'].get('name'))\n",
    "                row['has_email'] = bool(result['data'].get('email'))\n",
    "                skills = result['data'].get('skills', [])\n",
    "                row['has_skills'] = len(skills) > 0\n",
    "                row['num_skills'] = len(skills)\n",
    "            \n",
    "            data.append(row)\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"Calculate evaluation metrics\"\"\"\n",
    "        if len(self.df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        metrics = {\n",
    "            'total_files': len(self.df),\n",
    "            'parse_success_rate': self.df['success'].mean() * 100,\n",
    "            'name_extraction_rate': self.df['has_name'].mean() * 100,\n",
    "            'email_extraction_rate': self.df['has_email'].mean() * 100,\n",
    "            'skills_extraction_rate': self.df['has_skills'].mean() * 100,\n",
    "            'avg_skills_per_resume': self.df['num_skills'].mean(),\n",
    "            'max_skills_extracted': self.df['num_skills'].max(),\n",
    "            'min_skills_extracted': self.df['num_skills'].min()\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def display_report(self):\n",
    "        \"\"\"Display comprehensive evaluation report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PARSER EVALUATION REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        metrics = self.get_metrics()\n",
    "        \n",
    "        if not metrics:\n",
    "            print(\"No data to evaluate.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n Overall Performance:\")\n",
    "        print(f\"  • Total Files Processed: {metrics['total_files']}\")\n",
    "        print(f\"  • Parse Success Rate: {metrics['parse_success_rate']:.1f}%\")\n",
    "        \n",
    "        print(\"\\n Field Extraction Rates:\")\n",
    "        print(f\"  • Name Extraction: {metrics['name_extraction_rate']:.1f}%\")\n",
    "        print(f\"  • Email Extraction: {metrics['email_extraction_rate']:.1f}%\")\n",
    "        print(f\"  • Skills Extraction: {metrics['skills_extraction_rate']:.1f}%\")\n",
    "        \n",
    "        print(\"\\n Skills Analysis:\")\n",
    "        print(f\"  • Average Skills per Resume: {metrics['avg_skills_per_resume']:.1f}\")\n",
    "        print(f\"  • Maximum Skills Extracted: {metrics['max_skills_extracted']}\")\n",
    "        print(f\"  • Minimum Skills Extracted: {metrics['min_skills_extracted']}\")\n",
    "        \n",
    "        # Error analysis\n",
    "        errors = self.df[self.df['success'] == False]['error'].value_counts()\n",
    "        if len(errors) > 0:\n",
    "            print(\"\\n Error Analysis:\")\n",
    "            for error, count in errors.items():\n",
    "                if error:\n",
    "                    print(f\"  • {error}: {count} occurrence(s)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Evaluate results\n",
    "if main_parser.results:\n",
    "    evaluator = ParserEvaluator(main_parser.results)\n",
    "    evaluator.display_report()\n",
    "    \n",
    "    # Show detailed results\n",
    "    print(\"\\n Detailed Results:\")\n",
    "    display_df = evaluator.df[['file_name', 'success', 'has_name', 'has_email', 'num_skills']]\n",
    "    print(display_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No results to evaluate. Please run the parser first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. API Interface & Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Usage Example:\n",
      "========================================\n",
      "\n",
      "To parse a resume, use:\n",
      "result = parse_resume_file('path/to/resume.pdf')\n",
      "print(json.dumps(result, indent=2))\n",
      "\n",
      "Testing with: MathangiC_Resume.pdf\n",
      "Google GenAI client initialized with model: gemini-2.0-flash-exp\n",
      "Processing: MathangiC_Resume.pdf\n",
      "✓ Successfully parsed: MathangiC_Resume.pdf\n",
      "{\n",
      "  \"name\": \"Mathangi Chand\",\n",
      "  \"email\": \"chand53@uwo.ca\",\n",
      "  \"skills\": [\n",
      "    \"Python\",\n",
      "    \"SQL\",\n",
      "    \"Java\",\n",
      "    \"Spring\",\n",
      "    \"JavaScript\",\n",
      "    \"React\",\n",
      "    \"Angular\",\n",
      "    \"PostgreSQL\",\n",
      "    \"DB2\",\n",
      "    \"Salesforce CRM\",\n",
      "    \"Splunk\",\n",
      "    \"GCP\",\n",
      "    \"Docker\",\n",
      "    \"Kubernetes\",\n",
      "    \"Git\",\n",
      "    \"Linux\",\n",
      "    \"Scrum\",\n",
      "    \"Jira\",\n",
      "    \"Agile\",\n",
      "    \"ETL\",\n",
      "    \"CI/CD\",\n",
      "    \"Maven\",\n",
      "    \"Jenkins\",\n",
      "    \"Postman\",\n",
      "    \"RESTful APIs\",\n",
      "    \"HTML\",\n",
      "    \"CSS\",\n",
      "    \"HTTP\",\n",
      "    \"MVC\",\n",
      "    \"Microservices\",\n",
      "    \"Unit Testing\",\n",
      "    \"Hadoop\",\n",
      "    \"Kafka\",\n",
      "    \"Apache Spark\",\n",
      "    \"SDLC\",\n",
      "    \"Application Integration\",\n",
      "    \"Data Modeling\",\n",
      "    \"System Monitoring\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def parse_resume_file(file_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Simple API function to parse a single resume\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to resume file (PDF or DOCX)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with name, email, and skills\n",
    "    \"\"\"\n",
    "    parser = ResumeParser()\n",
    "    result = parser.parse_resume(file_path)\n",
    "    \n",
    "    if result['success']:\n",
    "        return result['data']\n",
    "    else:\n",
    "        # Return partial data if available\n",
    "        if result['data']:\n",
    "            return result['data']\n",
    "        else:\n",
    "            return {\n",
    "                'name': '',\n",
    "                'email': '',\n",
    "                'skills': [],\n",
    "                'error': result.get('error', 'Parsing failed')\n",
    "            }\n",
    "\n",
    "print(\"API Usage Example:\")\n",
    "print(\"=\"*40)\n",
    "print(\"\\nTo parse a resume, use:\")\n",
    "print(\"result = parse_resume_file('path/to/resume.pdf')\")\n",
    "print(\"print(json.dumps(result, indent=2))\")\n",
    "\n",
    "# Demonstrate with a test if files exist\n",
    "test_files = list(config.SAMPLE_DIR.glob('*.pdf')) + list(config.SAMPLE_DIR.glob('*.docx'))\n",
    "if test_files:\n",
    "    print(f\"\\nTesting with: {test_files[0].name}\")\n",
    "    test_result = parse_resume_file(test_files[0])\n",
    "    print(json.dumps(test_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices & Production Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices implementation\n",
    "class ProductionResumeParser(ResumeParser):\n",
    "    \"\"\"\n",
    "    Production-ready resume parser with additional features:\n",
    "    - Caching for repeated parses\n",
    "    - Rate limiting for API calls\n",
    "    - Enhanced error handling\n",
    "    - Logging support\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cache = {}\n",
    "        self.api_calls = 0\n",
    "        self.max_api_calls = 1500  # Gemini free tier limit per minute\n",
    "    \n",
    "    def parse_with_cache(self, file_path: Union[str, Path]) -> Dict:\n",
    "        \"\"\"Parse with caching support\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        cache_key = f\"{file_path.name}_{file_path.stat().st_mtime}\"\n",
    "        \n",
    "        if cache_key in self.cache:\n",
    "            print(f\"Using cached result for {file_path.name}\")\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        result = self.parse_resume(file_path)\n",
    "        self.cache[cache_key] = result\n",
    "        self.api_calls += 1\n",
    "        return result\n",
    "    \n",
    "    def validate_api_limits(self) -> bool:\n",
    "        \"\"\"Check if API rate limits are respected\"\"\"\n",
    "        return self.api_calls < self.max_api_calls\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best practices Considerations\n",
    "\n",
    "* Implement caching for repeated parses: Saves time and resources by storing results of previous API calls.\n",
    "\n",
    "* Add rate limiting for API calls (Gemini: 1500 req/min free tier): Prevents exceeding API limits and ensures fair usage.\n",
    "\n",
    "* Implement comprehensive error handling: Makes the application robust by gracefully managing unexpected issues.\n",
    "\n",
    "* Add logging for debugging: Helps developers track the application's behavior and diagnose problems.\n",
    "\n",
    "* Validate and sanitize all inputs: Crucial for security and preventing malicious data from being processed.\n",
    "\n",
    "* Consider async processing for bulk operations: Improves performance by allowing simultaneous API calls.\n",
    "\n",
    "* Implement retry logic for API failures: Ensures reliability by automatically retrying failed requests.\n",
    "\n",
    "* Add monitoring and metrics collection: Provides insights into the application's health and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
